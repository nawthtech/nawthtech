#!/bin/bash

# ============================================
# NawthTech AI Models Installation Script
# ============================================
# Ù‡Ø°Ø§ Ø§Ù„Ù…Ù„Ù Ù„ØªØ«Ø¨ÙŠØª Ø¬Ù…ÙŠØ¹ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…Ø¬Ø§Ù†ÙŠØ©
# ============================================

set -e  # Ø¥ÙŠÙ‚Ø§Ù Ø¹Ù†Ø¯ Ø­Ø¯ÙˆØ« Ø®Ø·Ø£

# Ø£Ù„ÙˆØ§Ù† Ù„Ù„Ø®Ø±ÙˆØ¬
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Ø¯Ø§Ù„Ø© Ø·Ø¨Ø§Ø¹Ø©
print_success() {
    echo -e "${GREEN}âœ… $1${NC}"
}

print_info() {
    echo -e "${BLUE}â„¹ï¸  $1${NC}"
}

print_warning() {
    echo -e "${YELLOW}âš ï¸  $1${NC}"
}

print_error() {
    echo -e "${RED}âŒ $1${NC}"
}

# Ø¯Ø§Ù„Ø© Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØªØ«Ø¨ÙŠØª
check_installed() {
    if command -v $1 &> /dev/null; then
        return 0
    else
        return 1
    fi
}

# ============================================
# Ø§Ù„ØªØ«Ø¨ÙŠØª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
# ============================================

main() {
    clear
    echo -e "${BLUE}"
    echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
    echo "â•‘      NawthTech AI Models Installer          â•‘"
    echo "â•‘     Ù…Ù†ØµØ© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù„Ù„Ù†Ù…Ùˆ Ø§Ù„Ø±Ù‚Ù…ÙŠ      â•‘"
    echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo -e "${NC}"
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… root
    if [ "$EUID" -eq 0 ]; then 
        print_warning "ØªØ´ØºÙŠÙ„ ÙƒÙ€ root! Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ù‡Ø°Ø§ Ø®Ø·ÙŠØ±Ø§Ù‹."
        read -p "Ù‡Ù„ ØªØ±ÙŠØ¯ Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø©ØŸ (y/n): " -n 1 -r
        echo
        if [[ ! $REPLY =~ ^[Yy]$ ]]; then
            exit 1
        fi
    fi
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ù…Ø´Ø±ÙˆØ¹
    print_info "Ø¥Ù†Ø´Ø§Ø¡ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª..."
    mkdir -p ./ai_models
    mkdir -p ./ai_models/text
    mkdir -p ./ai_models/image
    mkdir -p ./ai_models/video
    mkdir -p ./ai_models/audio
    mkdir -p ./data/ai/cache
    print_success "ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª"
    
    # Ø§Ù„Ù‚Ø³Ù… 1: ØªØ«Ø¨ÙŠØª Ollama (Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù†ØµÙŠØ©)
    install_ollama
    
    # Ø§Ù„Ù‚Ø³Ù… 2: ØªØ«Ø¨ÙŠØª Ù†Ù…Ø§Ø°Ø¬ Hugging Face
    install_huggingface_models
    
    # Ø§Ù„Ù‚Ø³Ù… 3: ØªØ«Ø¨ÙŠØª Stable Diffusion Ù„Ù„ØµÙˆØ±
    install_stable_diffusion
    
    # Ø§Ù„Ù‚Ø³Ù… 4: ØªØ«Ø¨ÙŠØª Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØµÙˆØª
    install_audio_models
    
    # Ø§Ù„Ù‚Ø³Ù… 5: Ø¥Ø¹Ø¯Ø§Ø¯ Environment Variables
    setup_environment
    
    # Ø§Ù„Ù‚Ø³Ù… 6: Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªØ«Ø¨ÙŠØª
    test_installation
    
    print_success "âœ… ØªÙ… ØªØ«Ø¨ÙŠØª Ø¬Ù…ÙŠØ¹ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¨Ù†Ø¬Ø§Ø­!"
    echo ""
    print_info "ðŸ”§ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©:"
    echo "1. Ø£Ø¶Ù Ù…ÙØ§ØªÙŠØ­ API ÙÙŠ Ù…Ù„Ù .env"
    echo "2. Ø´ØºÙ‘Ù„: docker-compose up -d"
    echo "3. Ø§ÙØªØ­: http://localhost:3000"
    echo ""
    print_info "ðŸ“ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯:"
    tree -L 2 ./ai_models
}

# ============================================
# 1. ØªØ«Ø¨ÙŠØª Ollama
# ============================================

install_ollama() {
    echo ""
    echo -e "${BLUE}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
    print_info "ØªØ«Ø¨ÙŠØª Ollama Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù†ØµÙŠØ© Ø§Ù„Ù…Ø­Ù„ÙŠØ©..."
    echo -e "${BLUE}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ollama
    if check_installed ollama; then
        print_success "Ollama Ù…Ø«Ø¨Øª Ù…Ø³Ø¨Ù‚Ø§Ù‹"
    else
        print_info "Ø¬Ø§Ø±Ù ØªØ«Ø¨ÙŠØª Ollama..."
        
        # ØªØ«Ø¨ÙŠØª Ollama (ÙŠØ¯Ø¹Ù… Linux Ùˆ macOS)
        if [[ "$OSTYPE" == "linux-gnu"* ]]; then
            # Linux
            curl -fsSL https://ollama.com/install.sh | sh
        elif [[ "$OSTYPE" == "darwin"* ]]; then
            # macOS
            /bin/bash -c "$(curl -fsSL https://ollama.com/install.sh)"
        else
            print_error "Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ´ØºÙŠÙ„ ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…: $OSTYPE"
            exit 1
        fi
        
        print_success "ØªÙ… ØªØ«Ø¨ÙŠØª Ollama"
    fi
    
    # ØªØ´ØºÙŠÙ„ Ø®Ø¯Ù…Ø© Ollama
    print_info "ØªØ´ØºÙŠÙ„ Ø®Ø¯Ù…Ø© Ollama..."
    sudo systemctl enable ollama
    sudo systemctl start ollama
    
    # Ø§Ù†ØªØ¸Ø§Ø± Ø­ØªÙ‰ ÙŠØ¨Ø¯Ø£ Ø§Ù„Ø®Ø¯Ù…Ø©
    sleep 5
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù†ØµÙŠØ©
    print_info "Ø¬Ø§Ø±Ù ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù†ØµÙŠØ©..."
    
    local models=(
        "llama3.2:3b"           # 3B parameters - Ø³Ø±ÙŠØ¹
        "mistral:7b"            # 7B - Ø¬ÙŠØ¯ Ù„Ù„ØªÙˆÙ„ÙŠØ¯
        "qwen2.5:7b"           # 7B - Ø¯Ø¹Ù… Ø¹Ø±Ø¨ÙŠ Ù…Ù…ØªØ§Ø²
        "phi3:mini"            # 3.8B - ÙØ¹Ø§Ù„
        "gemma:7b"             # Ù…Ù† Google
        "llama3.2:1b"          # 1B - Ø®ÙÙŠÙ Ø¬Ø¯Ø§Ù‹
    )
    
    for model in "${models[@]}"; do
        print_info "ØªØ­Ù…ÙŠÙ„: $model"
        ollama pull $model || print_warning "ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ $modelØŒ ØªØ®Ø·ÙŠ..."
    done
    
    print_success "ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù†ØµÙŠØ©"
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø®ØµØµ Ù„Ù€ NawthTech
    print_info "Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ NawthTech Ø§Ù„Ù…Ø®ØµØµ..."
    cat > ./ai_models/nawthtech-model.Modelfile << 'EOF'
FROM llama3.2:3b

# System Prompt Ù…Ø®ØµØµ Ù„Ù€ NawthTech
SYSTEM """
Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙÙŠ NawthTech - Ù…Ù†ØµØ© Ø§Ù„Ù†Ù…Ùˆ Ø§Ù„Ø±Ù‚Ù…ÙŠ.
Ù…ØªØ®ØµØµ ÙÙŠ:
1. Ø§Ù„ØªØ³ÙˆÙŠÙ‚ Ø§Ù„Ø±Ù‚Ù…ÙŠ ÙˆØ§Ù„Ù†Ù…Ùˆ
2. Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„Ø£Ø¹Ù…Ø§Ù„
3. ÙƒØªØ§Ø¨Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©
4. ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³ÙˆÙ‚ ÙˆØ§Ù„Ù…Ù†Ø§ÙØ³ÙŠÙ†
5. Ù†ØµØ§Ø¦Ø­ Ù„Ù„Ø´Ø±ÙƒØ§Øª Ø§Ù„Ù†Ø§Ø´Ø¦Ø©

ÙƒÙ† Ù…ÙÙŠØ¯Ø§Ù‹ØŒ Ø¯Ù‚ÙŠÙ‚Ø§Ù‹ØŒ ÙˆÙ…Ø±ÙƒØ²Ø§Ù‹ Ø¹Ù„Ù‰ ØªÙ‚Ø¯ÙŠÙ… Ø­Ù„ÙˆÙ„ Ø¹Ù…Ù„ÙŠØ©.
Ø§Ø³ØªØ®Ø¯Ù… Ù„ØºØ© ÙˆØ§Ø¶Ø­Ø© ÙˆØ§Ø­ØªØ±Ø§ÙÙŠØ©.
"""

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER num_ctx 4096
EOF
    
    ollama create nawthtech -f ./ai_models/nawthtech-model.Modelfile
    print_success "ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ NawthTech Ø§Ù„Ù…Ø®ØµØµ"
}

# ============================================
# 2. ØªØ«Ø¨ÙŠØª Ù†Ù…Ø§Ø°Ø¬ Hugging Face
# ============================================

install_huggingface_models() {
    echo ""
    echo -e "${BLUE}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
    print_info "ØªØ«Ø¨ÙŠØª Ù†Ù…Ø§Ø°Ø¬ Hugging Face..."
    echo -e "${BLUE}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Python Ùˆ pip
    if ! check_installed python3; then
        print_error "Python3 ØºÙŠØ± Ù…Ø«Ø¨Øª"
        print_info "Ø¬Ø§Ø±Ù ØªØ«Ø¨ÙŠØª Python3..."
        sudo apt-get update
        sudo apt-get install -y python3 python3-pip
    fi
    
    # ØªØ«Ø¨ÙŠØª Hugging Face CLI
    print_info "ØªØ«Ø¨ÙŠØª Hugging Face CLI..."
    pip3 install huggingface-hub
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
    mkdir -p ./ai_models/huggingface
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ÙÙŠØ¯Ø©
    print_info "Ø¬Ø§Ø±Ù ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù…Ù† Hugging Face..."
    
    # Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ±Ø¬Ù…Ø© (Ø¹Ø±Ø¨ÙŠ-Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ)
    print_info "ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ±Ø¬Ù…Ø©..."
    huggingface-cli download \
        "Helsinki-NLP/opus-mt-ar-en" \
        --local-dir ./ai_models/huggingface/translation-ar-en \
        --local-dir-use-symlinks False
    
    # Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙ„Ø®ÙŠØµ
    print_info "ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙ„Ø®ÙŠØµ..."
    huggingface-cli download \
        "facebook/bart-large-cnn" \
        --local-dir ./ai_models/huggingface/summarization \
        --local-dir-use-symlinks False
    
    # Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØµÙ†ÙŠÙ
    print_info "ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØµÙ†ÙŠÙ..."
    huggingface-cli download \
        "distilbert-base-uncased-finetuned-sst-2-english" \
        --local-dir ./ai_models/huggingface/sentiment \
        --local-dir-use-symlinks False
    
    print_success "ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…Ø§Ø°Ø¬ Hugging Face"
}

# ============================================
# 3. ØªØ«Ø¨ÙŠØª Stable Diffusion
# ============================================

install_stable_diffusion() {
    echo ""
    echo -e "${BLUE}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
    print_info "ØªØ«Ø¨ÙŠØª Stable Diffusion Ù„Ù„ØµÙˆØ±..."
    echo -e "${BLUE}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Stable Diffusion
    mkdir -p ./ai_models/stable-diffusion
    
    print_info "ØªØ­Ù…ÙŠÙ„ Stable Diffusion XL..."
    huggingface-cli download \
        "stabilityai/stable-diffusion-xl-base-1.0" \
        --local-dir ./ai_models/stable-diffusion/sdxl \
        --local-dir-use-symlinks False \
        --exclude "*.safetensors" \
        --exclude "*.ckpt"
    
    # ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø£ØµØºØ± Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
    print_info "ØªØ­Ù…ÙŠÙ„ Stable Diffusion 2.1 (Ø£ØµØºØ±)..."
    huggingface-cli download \
        "stabilityai/stable-diffusion-2-1" \
        --local-dir ./ai_models/stable-diffusion/sd2.1 \
        --local-dir-use-symlinks False \
        --exclude "*.safetensors" \
        --exclude "*.ckpt"
    
    # Ø¥Ù†Ø´Ø§Ø¡ Dockerfile Ù„Ù€ Stable Diffusion
    cat > ./ai_models/stable-diffusion/Dockerfile << 'EOF'
FROM pytorch/pytorch:2.1.0-cuda11.8-cudnn8-runtime

WORKDIR /app

# ØªØ«Ø¨ÙŠØª dependencies
RUN pip install --no-cache-dir \
    diffusers==0.24.0 \
    transformers==4.35.0 \
    accelerate==0.24.1 \
    torchvision==0.16.0 \
    pillow==10.1.0 \
    scipy==1.11.4 \
    flask==3.0.0

# Ù†Ø³Ø® Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø­Ù„ÙŠ
COPY sdxl/ /app/models/sdxl/

# Ø¥Ù†Ø´Ø§Ø¡ ØªØ·Ø¨ÙŠÙ‚ Flask Ø¨Ø³ÙŠØ·
COPY app.py /app/

EXPOSE 7860

CMD ["python", "app.py"]
EOF
    
    # Ø¥Ù†Ø´Ø§Ø¡ ØªØ·Ø¨ÙŠÙ‚ Flask
    cat > ./ai_models/stable-diffusion/app.py << 'EOF'
from flask import Flask, request, jsonify
from diffusers import StableDiffusionXLPipeline
import torch
from PIL import Image
import io
import base64

app = Flask(__name__)

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
print("Loading Stable Diffusion XL model...")
pipe = StableDiffusionXLPipeline.from_pretrained(
    "/app/models/sdxl",
    torch_dtype=torch.float16,
    use_safetensors=True,
    variant="fp16"
)
pipe.to("cuda" if torch.cuda.is_available() else "cpu")
print("Model loaded successfully!")

@app.route('/health', methods=['GET'])
def health():
    return jsonify({"status": "healthy", "model": "stable-diffusion-xl"})

@app.route('/generate', methods=['POST'])
def generate():
    try:
        data = request.json
        prompt = data.get('prompt', '')
        
        if not prompt:
            return jsonify({"error": "Prompt is required"}), 400
        
        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØ±Ø©
        image = pipe(
            prompt=prompt,
            num_inference_steps=25,
            guidance_scale=7.5
        ).images[0]
        
        # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ base64
        buffered = io.BytesIO()
        image.save(buffered, format="PNG")
        img_str = base64.b64encode(buffered.getvalue()).decode()
        
        return jsonify({
            "success": True,
            "image": f"data:image/png;base64,{img_str}",
            "prompt": prompt
        })
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=7860)
EOF
    
    print_success "ØªÙ… Ø¥Ø¹Ø¯Ø§Ø¯ Stable Diffusion"
}

# ============================================
# 4. ØªØ«Ø¨ÙŠØª Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØµÙˆØª
# ============================================

install_audio_models() {
    echo ""
    echo -e "${BLUE}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
    print_info "ØªØ«Ø¨ÙŠØª Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØµÙˆØª..."
    echo -e "${BLUE}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
    
    mkdir -p ./ai_models/audio
    
    # ØªØ­Ù…ÙŠÙ„ Whisper Ù„Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØª
    print_info "ØªØ­Ù…ÙŠÙ„ Whisper (OpenAI)..."
    huggingface-cli download \
        "openai/whisper-medium" \
        --local-dir ./ai_models/audio/whisper \
        --local-dir-use-symlinks False
    
    # ØªØ­Ù…ÙŠÙ„ Bark Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØª
    print_info "ØªØ­Ù…ÙŠÙ„ Bark (Suno AI)..."
    huggingface-cli download \
        "suno/bark" \
        --local-dir ./ai_models/audio/bark \
        --local-dir-use-symlinks False
    
    # ØªØ­Ù…ÙŠÙ„ XTTS Ù„Ù„ØµÙˆØª Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù„ØºØ§Øª
    print_info "ØªØ­Ù…ÙŠÙ„ XTTS-v2 (Coqui AI)..."
    huggingface-cli download \
        "coqui/XTTS-v2" \
        --local-dir ./ai_models/audio/xtts \
        --local-dir-use-symlinks False
    
    # Ø¥Ù†Ø´Ø§Ø¡ Dockerfile Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„ØµÙˆØª
    cat > ./ai_models/audio/Dockerfile << 'EOF'
FROM python:3.10-slim

WORKDIR /app

# ØªØ«Ø¨ÙŠØª system dependencies
RUN apt-get update && apt-get install -y \
    ffmpeg \
    libsndfile1 \
    && rm -rf /var/lib/apt/lists/*

# ØªØ«Ø¨ÙŠØª Python packages
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Ù†Ø³Ø® Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
COPY whisper/ /app/models/whisper/
COPY bark/ /app/models/bark/
COPY xtts/ /app/models/xtts/

# Ù†Ø³Ø® ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØµÙˆØª
COPY audio_app.py /app/

EXPOSE 7861

CMD ["python", "audio_app.py"]
EOF
    
    # Ø¥Ù†Ø´Ø§Ø¡ requirements.txt Ù„Ù„ØµÙˆØª
    cat > ./ai_models/audio/requirements.txt << 'EOF'
openai-whisper==20231117
TTS==0.22.0
torch==2.1.0
torchaudio==2.1.0
flask==3.0.0
numpy==1.24.3
scipy==1.11.4
soundfile==0.12.1
EOF
    
    # Ø¥Ù†Ø´Ø§Ø¡ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØµÙˆØª
    cat > ./ai_models/audio/audio_app.py << 'EOF'
from flask import Flask, request, jsonify
import whisper
import torch
import io
import base64
from TTS.api import TTS

app = Flask(__name__)

# ØªØ­Ù…ÙŠÙ„ Whisper
print("Loading Whisper model...")
whisper_model = whisper.load_model("/app/models/whisper")
print("Whisper loaded!")

# ØªØ­Ù…ÙŠÙ„ TTS
print("Loading TTS models...")
tts = TTS(model_name="tts_models/multilingual/multi-dataset/xtts_v2", progress_bar=False)
print("TTS loaded!")

@app.route('/health', methods=['GET'])
def health():
    return jsonify({"status": "healthy"})

@app.route('/transcribe', methods=['POST'])
def transcribe():
    try:
        if 'audio' not in request.files:
            return jsonify({"error": "No audio file"}), 400
        
        audio_file = request.files['audio']
        
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ
        result = whisper_model.transcribe(audio_file)
        
        return jsonify({
            "success": True,
            "text": result["text"],
            "language": result["language"]
        })
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/tts', methods=['POST'])
def text_to_speech():
    try:
        data = request.json
        text = data.get('text', '')
        language = data.get('language', 'en')
        
        if not text:
            return jsonify({"error": "Text is required"}), 400
        
        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØª
        audio_path = "/tmp/output.wav"
        tts.tts_to_file(
            text=text,
            speaker_wav=None,  # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØµÙˆØª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ
            language=language,
            file_path=audio_path
        )
        
        # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù ÙˆØ¥Ø±Ø¬Ø§Ø¹Ù‡
        with open(audio_path, 'rb') as f:
            audio_data = f.read()
        
        audio_b64 = base64.b64encode(audio_data).decode()
        
        return jsonify({
            "success": True,
            "audio": f"data:audio/wav;base64,{audio_b64}",
            "text": text,
            "language": language
        })
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=7861)
EOF
    
    print_success "ØªÙ… Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØµÙˆØª"
}

# ============================================
# 5. Ø¥Ø¹Ø¯Ø§Ø¯ Environment Variables
# ============================================

setup_environment() {
    echo ""
    echo -e "${BLUE}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
    print_info "Ø¥Ø¹Ø¯Ø§Ø¯ Environment Variables..."
    echo -e "${BLUE}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù .env
    cat > .env << 'EOF'
# ============================================
# NawthTech AI Environment Configuration
# ============================================

# Ollama Configuration
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=nawthtech
OLLAMA_KEEP_ALIVE=5m

# Hugging Face (Ø§Ø­ØµÙ„ Ø¹Ù„Ù‰ Token Ù…Ù†: https://huggingface.co/settings/tokens)
HUGGINGFACE_TOKEN=your_huggingface_token_here

# Google Gemini (Ù…Ø¬Ø§Ù†ÙŠ: https://makersuite.google.com/app/apikey)
GEMINI_API_KEY=your_gemini_api_key_here

# Stability AI (25 ØµÙˆØ±Ø© Ù…Ø¬Ø§Ù†ÙŠØ©/Ø´Ù‡Ø±: https://platform.stability.ai/)
STABILITY_API_KEY=your_stability_key_here

# Model Paths
AI_MODELS_PATH=./ai_models
AI_CACHE_PATH=./data/ai/cache
AI_DATA_PATH=./data/ai

# Service Ports
OLLAMA_PORT=11434
STABLE_DIFFUSION_PORT=7860
AUDIO_SERVICE_PORT=7861
BACKEND_PORT=8080
FRONTEND_PORT=3000

# AI Configuration
DEFAULT_TEXT_MODEL=gemini-2.0-flash
DEFAULT_IMAGE_MODEL=stable-diffusion-xl
DEFAULT_AUDIO_MODEL=whisper-medium
DEFAULT_TRANSLATION_MODEL=opus-mt-ar-en

# Rate Limits (Free Tier)
MAX_REQUESTS_PER_MINUTE=30
MAX_REQUESTS_PER_DAY=1000
MAX_IMAGES_PER_DAY=10
MAX_VIDEOS_PER_DAY=3

# User Quotas (Free Tier)
FREE_USER_QUOTA_TEXT=10000      # ÙƒÙ„Ù…Ø§Øª/Ø´Ù‡Ø±
FREE_USER_QUOTA_IMAGES=10       # ØµÙˆØ±/Ø´Ù‡Ø±
FREE_USER_QUOTA_VIDEOS=3        # ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª/Ø´Ù‡Ø±
FREE_USER_QUOTA_AUDIO=30        # Ø¯Ù‚Ø§Ø¦Ù‚/Ø´Ù‡Ø±

# Cache Settings
AI_CACHE_TTL=24h
AI_CACHE_MAX_SIZE=10GB

# Logging
AI_LOG_LEVEL=info
AI_LOG_PATH=./logs/ai.log

# Monitoring
ENABLE_AI_METRICS=true
AI_METRICS_PORT=9090
EOF
    
    print_success "ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù .env"
    print_warning "âš ï¸  ÙŠØ±Ø¬Ù‰ ØªØ¹Ø¯ÙŠÙ„ Ù…Ù„Ù .env ÙˆØ¥Ø¶Ø§ÙØ© Ù…ÙØ§ØªÙŠØ­ API Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ"
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù docker-compose.yml Ù„Ù„Ù€ AI services
    cat > docker-compose.ai.yml << 'EOF'
version: '3.8'

services:
  # Ollama Service
  ollama:
    image: ollama/ollama:latest
    container_name: nawthtech-ollama
    ports:
      - "11434:11434"
    volumes:
      - ./ai_models/ollama:/root/.ollama
      - ./ai_models/nawthtech-model.Modelfile:/root/nawthtech.Modelfile
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_KEEP_ALIVE=5m
    restart: unless-stopped
    networks:
      - ai-network
    command: >
      sh -c "
        ollama serve &
        sleep 10 &&
        ollama create nawthtech -f /root/nawthtech.Modelfile
        wait
      "
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  # Stable Diffusion Service
  stable-diffusion:
    build: ./ai_models/stable-diffusion
    container_name: nawthtech-sd
    ports:
      - "7860:7860"
    volumes:
      - ./ai_models/stable-diffusion/sdxl:/app/models/sdxl
      - ./data/ai/cache:/root/.cache
    environment:
      - HF_TOKEN=${HUGGINGFACE_TOKEN}
      - MODEL_PATH=/app/models/sdxl
    restart: unless-stopped
    networks:
      - ai-network
    depends_on:
      - ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Audio Service
  audio-service:
    build: ./ai_models/audio
    container_name: nawthtech-audio
    ports:
      - "7861:7861"
    volumes:
      - ./ai_models/audio:/app/models
      - ./data/ai/cache:/root/.cache
    environment:
      - HF_TOKEN=${HUGGINGFACE_TOKEN}
    restart: unless-stopped
    networks:
      - ai-network

  # AI Gateway (Reverse Proxy)
  ai-gateway:
    image: nginx:alpine
    container_name: nawthtech-ai-gateway
    ports:
      - "8000:80"
    volumes:
      - ./nginx/ai-gateway.conf:/etc/nginx/conf.d/default.conf
    restart: unless-stopped
    networks:
      - ai-network
    depends_on:
      - ollama
      - stable-diffusion
      - audio-service

networks:
  ai-network:
    driver: bridge

volumes:
  ollama_data:
  sd_cache:
  audio_cache:
EOF
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ nginx Ù„Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
    mkdir -p nginx
    
    cat > nginx/ai-gateway.conf << 'EOF'
server {
    listen 80;
    server_name localhost;
    
    # Health check endpoint
    location /health {
        return 200 '{"status": "healthy"}';
        add_header Content-Type application/json;
    }
    
    # Ollama API
    location /api/ollama/ {
        proxy_pass http://ollama:11434/;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        
        # Ø²ÙŠØ§Ø¯Ø© timeouts Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ÙƒØ¨ÙŠØ±Ø©
        proxy_read_timeout 300s;
        proxy_connect_timeout 300s;
        proxy_send_timeout 300s;
    }
    
    # Stable Diffusion API
    location /api/sd/ {
        proxy_pass http://stable-diffusion:7860/;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    }
    
    # Audio Service API
    location /api/audio/ {
        proxy_pass http://audio-service:7861/;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    }
    
    # Default
    location / {
        return 404 '{"error": "Not found"}';
        add_header Content-Type application/json;
    }
}
EOF
    
    print_success "ØªÙ… Ø¥Ø¹Ø¯Ø§Ø¯ Docker Compose Ù„Ù„Ù€ AI Services"
}

# ============================================
# 6. Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªØ«Ø¨ÙŠØª
# ============================================

test_installation() {
    echo ""
    echo -e "${BLUE}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
    print_info "Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªØ«Ø¨ÙŠØª..."
    echo -e "${BLUE}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
    
    # Ø§Ø®ØªØ¨Ø§Ø± Ollama
    print_info "Ø§Ø®ØªØ¨Ø§Ø± Ollama..."
    if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
        print_success "Ollama ÙŠØ¹Ù…Ù„ Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­"
    else
        print_warning "Ollama ØºÙŠØ± Ù‚ÙŠØ¯ Ø§Ù„ØªØ´ØºÙŠÙ„. Ø¬Ø§Ø±Ù Ø§Ù„Ø¨Ø¯Ø¡..."
        sudo systemctl start ollama
        sleep 5
    fi
    
    # Ø§Ø®ØªØ¨Ø§Ø± Python packages
    print_info "Ø§Ø®ØªØ¨Ø§Ø± Python packages..."
    if python3 -c "import huggingface_hub" 2>/dev/null; then
        print_success "Hugging Face Hub Ù…Ø«Ø¨Øª"
    else
        print_warning "Ø¬Ø§Ø±Ù ØªØ«Ø¨ÙŠØª Hugging Face Hub..."
        pip3 install huggingface-hub
    fi
    
    # Ø§Ø®ØªØ¨Ø§Ø± Docker
    print_info "Ø§Ø®ØªØ¨Ø§Ø± Docker..."
    if docker --version > /dev/null 2>&1; then
        print_success "Docker Ù…Ø«Ø¨Øª"
    else
        print_error "Docker ØºÙŠØ± Ù…Ø«Ø¨Øª. ÙŠØ±Ø¬Ù‰ ØªØ«Ø¨ÙŠØªÙ‡ Ø£ÙˆÙ„Ø§Ù‹."
        print_info "ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„ØªØ«Ø¨ÙŠØª: https://docs.docker.com/engine/install/"
        exit 1
    fi
    
    # Ø§Ø®ØªØ¨Ø§Ø± Docker Compose
    if docker-compose --version > /dev/null 2>&1; then
        print_success "Docker Compose Ù…Ø«Ø¨Øª"
    else
        print_warning "Docker Compose ØºÙŠØ± Ù…Ø«Ø¨Øª. Ø¬Ø§Ø±Ù Ø§Ù„ØªØ«Ø¨ÙŠØª..."
        sudo curl -L "https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)" \
            -o /usr/local/bin/docker-compose
        sudo chmod +x /usr/local/bin/docker-compose
    fi
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù…Ø³Ø§Ø­Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†
    print_info "Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù…Ø³Ø§Ø­Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†..."
    free_space=$(df -h . | awk 'NR==2 {print $4}')
    print_info "Ø§Ù„Ù…Ø³Ø§Ø­Ø© Ø§Ù„Ø­Ø±Ø©: $free_space"
    
    if [[ ${free_space%G} -lt 20 ]]; then
        print_warning "âš ï¸  Ù…Ø³Ø§Ø­Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ù…Ù†Ø®ÙØ¶Ø©! ØªØ­ØªØ§Ø¬ 20GB Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ Ù„Ù„Ù†Ù…Ø§Ø°Ø¬."
    fi
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø© RAM
    print_info "Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø©..."
    total_ram=$(free -g | awk 'NR==2 {print $2}')
    print_info "Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„ÙƒÙ„ÙŠØ©: ${total_ram}GB"
    
    if [[ $total_ram -lt 8 ]]; then
        print_warning "âš ï¸  Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù…Ù†Ø®ÙØ¶Ø©! ØªØ­ØªØ§Ø¬ 8GB Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬."
    fi
    
    # Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ«Ø¨ÙŠØª
    create_installation_report
}

# ============================================
# Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ«Ø¨ÙŠØª
# ============================================

create_installation_report() {
    echo ""
    echo -e "${BLUE}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
    print_info "Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ«Ø¨ÙŠØª..."
    echo -e "${BLUE}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
    
    cat > INSTALLATION_REPORT.md << 'EOF'
# NawthTech AI Models Installation Report

## ðŸ“… ØªØ§Ø±ÙŠØ® Ø§Ù„ØªØ«Ø¨ÙŠØª
'$(date)'

## âœ… Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ù…Ø«Ø¨ØªØ©

### 1. Ollama (Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù†ØµÙŠØ©)
- âœ… llama3.2:3b
- âœ… mistral:7b  
- âœ… qwen2.5:7b
- âœ… phi3:mini
- âœ… gemma:7b
- âœ… nawthtech (Ù…Ø®ØµØµ)

### 2. Hugging Face Models
- âœ… Helsinki-NLP/opus-mt-ar-en (ØªØ±Ø¬Ù…Ø©)
- âœ… facebook/bart-large-cnn (ØªÙ„Ø®ÙŠØµ)
- âœ… distilbert-base-uncased-finetuned-sst-2-english (ØªØµÙ†ÙŠÙ)

### 3. Stable Diffusion (Ø§Ù„ØµÙˆØ±)
- âœ… stabilityai/stable-diffusion-xl-base-1.0
- âœ… stabilityai/stable-diffusion-2-1

### 4. Audio Models (Ø§Ù„ØµÙˆØª)
- âœ… openai/whisper-medium (ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù…)
- âœ… suno/bark (ØªÙˆÙ„ÙŠØ¯ ØµÙˆØª)
- âœ… coqui/XTTS-v2 (Ù†Øµ Ø¥Ù„Ù‰ ØµÙˆØª)

## ðŸš€ ÙƒÙŠÙÙŠØ© Ø§Ù„ØªØ´ØºÙŠÙ„

### Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 1: Ø§Ø³ØªØ®Ø¯Ø§Ù… Docker Compose
```bash
# ØªØ´ØºÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø®Ø¯Ù…Ø§Øª AI
docker-compose -f docker-compose.ai.yml up -d

# Ø¹Ø±Ø¶ Ø§Ù„Ø³Ø¬Ù„Ø§Øª
docker-compose -f docker-compose.ai.yml logs -f